## 10.10干的事：

1. 李沐的视频 ViT论文模型
2. 师兄推荐的论文
3. wct的代码





1 vit：直接使用标准nlp的transformer都可以实现cv操作（直接爆火，继alexnet后）

第一个论文：Intriguing Properties of Vision Transformers

第二个论文： An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale

![image-20221010104934346](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221010104934346.png)

tip：模型简单：2500天tpuv3训练时间 草

把transformer应用到视觉问题的难处：硬件支持序列长度最大512； 我们要怎么把2d的图片弄成一个1d 的序列；（224*224个像素点，太大了不可行）

于是有一些人进行了一定的革新（先cnn，把输出的特征拉平就行） 或者 在输入图像上加一个window，控制输入图片的大小，；或者在图片的H和W两轴方向分别做一个selfattention等等； 但没有办法在现在的硬件配置上进行加速，因此没有很好的改善什么；

于是本文想 直接使用传统nlptransformer模型，就不会很难加速，为此将图片分为多个patch（16*16）。

vit的稳健性还是很好的

除了一开始将图像打成patches，没有再对图像进行什么 inductive biases

vit ；DETR;SETR;  ViT-FRCNN 分别是classification、 segmentation、detection领域的模型‘

Swin-transformer  ；MAE 等经典模型  

![image-20221010134236540](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221010134236540.png)

 输入 196个token 每个token维度是768 =16\*16\*3 还有一个cls字符

本文顺带还很好地讲了一下实现细节。

![image-20221010140626621](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221010140626621.png)

包括每个向量多大 怎么连接等；

注意： 这个clstoken借鉴了bert模型，是一个可以学习的token

启示 vit必须要在大规模数据集才能取得比resnet更好的效果 因此 如何使用vit去做小样本学习是一个充满潜力的方向；

感觉vit也可以用在带时间戳的数据上 甚至是视频（一点自己的不成熟的想法）





3 transformer代码带读；首先 从输入和输出层面搞懂他在干什么，[代码来源](https://github.com/lanzehua/nlp-tutorial/tree/master/5-1.Transformer)

体会：可以先把大的框架搭建好，再去实现细节；第二就是一定要搞清楚每一步的数据流动方式 数据大小等；

[视频作者给代码加了很详细的注释！](https://www.bilibili.com/video/av336091126/)(自己可以在复习的时候看)





2 论文（Video to Video Synthesis)

​	我们研究了视频到视频的合成问题，其目标是从输入源视频（例如，语义分割掩码序列）学习映射函数到输出逼真视频，精确地描述源视频的内容。虽然其图像对应的图像到图像的转换问题是一个流行的话题，但视频到视频的合成问题在文献中探讨较少。在不对时间动态进行建模的情况下，直接将现有的图像合成方法应用于输入视频，往往会导致低视觉质量的时间不相干视频。在本文中，我们提出了一种在生成对抗学习框架下的视频到视频的合成方法。通过精心设计的生成器和鉴别器，再加上时空对抗目标，我们在分割掩模、草图和姿态等不同的输入格式上获得了高分辨率、逼真的、时间一致的视频结果。在多个基准测试上的实验表明，与强基线相比，我们的方法具有优势。特别是，我们的模型能够合成2K分辨率的街景视频，最长可达30秒的时间，这大大提高了最先进的视频合成水平。最后，我们将我们的方法应用于未来的视频预测，优于几个竞争的系统。代码、模型和更多的结果可以在我们的网站上找到。



我们将视频到视频的合成问题转换为一个分布匹配问题，其目标是训练一个模型，使给定输入视频的合成视频的条件分布类似于真实视频。为此，我们学习了一个有条件生成的对抗模型

我们在几个数据集上进行了广泛的实验，将一系列分割掩模转换为逼真的视频。定量和定性的结果表明，我们的合成镜头看起来比那些从强基线更逼真。例如，请参见[图1](![img](file:///C:\Users\lzhrt\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)https://tcwang0509.github.io/vid2vid/paper_gifs/cityscapes_comparison.gif)。我们进一步证明，所提出的方法可以生成逼真的2K分辨率的视频，长达30秒。我们的方法还允许用户对视频生成结果进行灵活的高级控制。例如，用户可以很容易地用街景视频中的树木替换所有的建筑物。此外，我们的方法适用于其他输入视频格式，如人脸草图和身体姿势，使许多应用程序从人脸转换到人体运动传输。最后，我们将我们的方法扩展到未来的预测，并表明我们的方法可以优于现有的系统。请访问我们的网站的代码，模型，和更多的结果。

对于image2image这个问题有大量的工作，[6,31,33,43,44,63,66,73,82,83]。我们的方法是他们的视频对应物。除了确保每个视频帧看起来逼真之外，视频合成模型还必须产生时间相干的帧，这是一项具有挑战性的任务，特别是对于一个长时间的视频。

无条件视频合成：最近的工作[59,67,69]扩展了无条件视频合成的GAN框架，它学习了一个将随机向量转换为视频的生成器。比如

VGAN [69]使用了一个时空卷积网络。TGAN [59]将一个潜在代码投射到一组潜在图像代码中，并使用一个图像生成器将这些潜在图像代码转换为帧。MoCoGAN [67]将潜在空间分解到运动子空间和内容子空间，并使用递归神经网络生成一系列运动码。由于无条件的设置，这些方法通常会产生低分辨率和短长度的视频

Future video prediction
用观测到的帧预测未来的帧，它以图像重建loss 进行优化，常常会导致模糊，同时也无法完成长时间的预测毕竟信息有限。

video-to-video合成问题与视频预测在本质上是不同的。因为video-to-video合成不会尝试预测物体的运动，而是根据已有的条件信息来产生另一个域的对应信息。
视频去噪、去模糊、去雨等也可以看作是video-to-vide synthesis问题。但这类研究都是对特定任务的，因此不能直接把已有的去噪之类的方法拿来做为本文的方法。



具体做法： ![image-20221010183243235](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221010183243235.png)

然后所有的步骤都是为了求解这个minmax问题；

序列生成器：将条件分布写为上面那个乘积形式

![image-20221010190220745](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221010190220745.png)

![image-20221010190400282](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221010190400282.png)

视频信号在连续的帧中包含大量的冗余信息

.M为掩模预测网络。我们的遮挡掩码是软的，而不是二进制的，以更好地处理“放大”的场景。例如，当一个物体靠近我们的相机时，如果我们只扭曲之前的帧，这个物体就会随着时间的推移而变得更加模糊。为了提高物体的分辨率，我们需要合成新的纹理细节。通过使用软掩模，我们可以通过逐步混合扭曲的像素和新合成的像素来添加细节。![image-20221010191534882](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221010191534882.png)

While *DI *conditions on the source image, *DV* conditions on the flow.

![image-20221010193607963](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221010193607963.png)

multimodel synthesis： 合成网络F是一个单模型映射函数。给定一个输入源视频，它只能生成一个输出视频。为了实现多模态合成[19,73,83]，我们对一个由实例级语义分割掩码组成的源视频采用了特征嵌入方案[73]。具体来说，在训练时，我们训练一个图像编码器E将完全真实图像xt编码为d维特征图（在我们的实验中是d=3）。然后，我们对映射应用一个实例级的平均池化，从而使同一对象中的所有像素共享相同的特征向量。然后，我们将实例平均特征映射zt和输入语义分割掩模st输入给生成器f。一旦训练完成，我们将高斯分布的混合物拟合到属于同一对象类的特征向量上。在测试时，我们使用每个对象类的估计分布来为每个对象实例采样一个特征向量。给定不同的特征向量，生成器F可以合成具有不同视觉外观的视频。

![image-20221010195421704](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221010195421704.png)

建议搞一下这个的代码。该模型效果尚可

