## 10.11日做的事

---

1. 继续学习transformer源码
2. 一篇论文
3. 一篇昨天的论文（pix2pix升级版：High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs）
4. 高级体系结构的第一篇论文一点摘要（**Sparse ReRAM Engine: Joint Exploration of Activation and Weight Sparsity in Compressed Neural Networks**）
5. 有空的话跑一下gatys代码



1 一般来说 K V是差不多的，和Q不同，这就是attention，但是当QKV都一样的时候，着就成为了自注意力机制！

多头注意力内部的矩阵一定是一个方阵

用zip函数组合输入的qkv和线性层

```python
query,key value= \
[model(x).view(batch_size,-1,self.head, self.d_k).transpose(1,2) for model , x in zip(self.linears,(query, key , value))]
```

自己看[视频](https://www.bilibili.com/video/BV18S4y1E7QA/?p=46&spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=1916e0505d32e3020bf2d782c78e7ed0)吧方便回忆	



3 该论文作者做了如下操作：

![image-20221011155705149](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221011155705149.png)

我们的G的网络架构。我们首先在较低分辨率的图像上训练一个残差网络G1。然后，将另一个残差网络G2附加到G1上，并对高分辨率图像进行联合训练。具体来说，G2中残差块的输入是G2的特征图和G1的最后一个特征图的元素级之和。

![image-20221011155914428](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221011155914428.png)

总结来说，这篇paper的贡献点如下：

1、很好地解决了GAN难以生成高清图片的问题，可以产生十分真实 2K 高清图片

2、可以通过调整语义分割图片以及选取不同的特征，自由生成图片。

![image-20221011161101305](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221011161101305.png)

perceptual loss 的 pytorch 实现

```python

import torch
import torch.nn as nn
from torchvision import models
from PIL import Image
from style_transfer_perceptual_loss.image_dataset import get_transform
from src.utils.train_utils import get_device


class Vgg16(nn.Module):
    def __init__(self):
        super(Vgg16, self).__init__()
        features = models.vgg16(pretrained=True).features
        self.to_relu_1_2 = nn.Sequential()
        self.to_relu_2_2 = nn.Sequential()
        self.to_relu_3_3 = nn.Sequential()
        self.to_relu_4_3 = nn.Sequential()

        for x in range(4):
            self.to_relu_1_2.add_module(str(x), features[x])
        for x in range(4, 9):
            self.to_relu_2_2.add_module(str(x), features[x])
        for x in range(9, 16):
            self.to_relu_3_3.add_module(str(x), features[x])
        for x in range(16, 23):
            self.to_relu_4_3.add_module(str(x), features[x])

        # don't need the gradients, just want the features
        for param in self.parameters():
            param.requires_grad = False

    def forward(self, x):
        h = self.to_relu_1_2(x)
        h_relu_1_2 = h
        h = self.to_relu_2_2(h)
        h_relu_2_2 = h
        h = self.to_relu_3_3(h)
        h_relu_3_3 = h
        h = self.to_relu_4_3(h)
        h_relu_4_3 = h
        out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)
        return out


def gram(x):
    (bs, ch, h, w) = x.size()
    f = x.view(bs, ch, w * h)
    f_T = f.transpose(1, 2)
    G = f.bmm(f_T) / (ch * h * w)
    return G


class PerceptualLoss:
    def __init__(self, args):
        self.content_layer = args.content_layer
        device = get_device(args)
        self.vgg = nn.DataParallel(Vgg16())
        self.vgg.eval()
        self.mse = nn.DataParallel(nn.MSELoss())
        self.mse_sum = nn.DataParallel(nn.MSELoss(reduction='sum'))
        style_image = Image.open(args.style_image).convert('RGB')
        _, transform = get_transform(args)
        style_image = transform(style_image).repeat(args.batch_size, 1, 1, 1).to(device)

        with torch.no_grad():
            self.style_features = self.vgg(style_image)
            self.style_gram = [gram(fmap) for fmap in self.style_features]
        pass

    def __call__(self, x, y_hat):
        b, c, h, w = x.shape
        y_content_features = self.vgg(x)
        y_hat_features = self.vgg(y_hat)

        recon = y_content_features[self.content_layer]
        recon_hat = y_hat_features[self.content_layer]
        L_content = self.mse(recon_hat, recon)

        y_hat_gram = [gram(fmap) for fmap in y_hat_features]
        L_style = 0
        for j in range(len(y_content_features)):
            _, c_l, h_l, w_l = y_hat_features[j].shape
            L_style += self.mse_sum(y_hat_gram[j], self.style_gram[j]) / float(c_l * h_l * w_l)

        L_pixel = self.mse(y_hat, x)

        # calculate total variation regularization (anisotropic version)
        # https://www.wikiwand.com/en/Total_variation_denoising
        diff_i = torch.sum(torch.abs(y_hat[:, :, :, 1:] - y_hat[:, :, :, :-1]))
        diff_j = torch.sum(torch.abs(y_hat[:, :, 1:, :] - y_hat[:, :, :-1, :]))
        L_tv = (diff_i + diff_j) / float(c * h * w)

        return L_content, L_style, L_pixel, L_tv
```

![image-20221011161424945](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221011161424945.png)

*在这项工作中，我们生成了2048个×1024视觉上吸引人的结果，具有一个新的对抗性损失，以及新的多尺度生成器和鉴别器架构。*此外，我们还将我们的框架扩展到具有两个附加特性的交互式视觉操作。首先，我们合并了对象实例分割信息，这支持对象操作，如删除/添加对象和更改对象类别。其次，我们提出了一种方法，在相同的输入下生成不同的结果，允许用户交互地编辑对象外观。人类意见研究表明，我们的方法显著优于现有的方法，提高了深度图像合成和编辑的质量和分辨率。

以下是本文采用的方法：

**Instance-Level Image Synthesis**

- **3.2. Improving Photorealism and Resolution**
- - **Coarse-to-fifine generator** 
  - **Multi-scale discriminators**
  - **Improved adversarial loss** 
- **Improved adversarial loss** 
- - 相反，我们认为实例映射提供的最关键的信息是对象边界，而这在语义标签映射中是不可用的。例如，当同一个类的对象彼此相邻时，仅查看语义标签映射并不能区分它们。对于街景尤其如此，因为许多停放的汽车或行走的行人经常彼此相邻，如图4a所示。但是，使用实例映射，分离这些对象成为一个更容易的任务。因此，为了提取这一信息，我们首先计算实例边界图（图4b）。在我们的实现中，如果实例边界映射中的一个像素的对象ID与它的任何4个（KNN算法！）邻居不同，则该像素为1，否则为0。然后，将实例边界映射与语义标签映射的一个热向量表示连接起来，并输入生成器网络。类似地，鉴别器的输入是实例边界映射、语义标签映射和真实/合成图像的通道连接。图5b显示了一个演示通过使用对象边界进行改进的示例。
-  **Learning an Instance-level Feature Embedding**

![image-20221011164025878](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221011164025878.png)









4 **Sparse ReRAM Engine: Joint Exploration of Activation and Weight Sparsity in Compressed Neural Networks**

利用模型稀疏性减少无效计算是DNN推理加速器实现能量效率的常用方法。然而，由于crossbar结构的紧密耦合，利用基于ReRAM的神经网络加速器的稀疏性是一个较少探索的领域。基于ReRAM的NN加速器的现有体系结构研究假设，整个纵横制阵列可以在一个周期内激活。然而，出于推理精度的考虑，矩阵向量计算在实践中必须以更小的粒度进行，称为运算单元（OU）。基于OU的体系结构为利用DNN稀疏性创造了新的机会。在本文中，我们提出了第一个实用的稀疏ReRAM引擎，它利用了权重和激活稀疏性。我们的评估表明，所提出的方法在消除无效计算方面是有效的，并且提供了显著的性能改进和节能。

CONCLUSION:在本文中，我们研究了基于ReRAM的DNN加速器稀疏性探索的设计挑战，并证明了一个实用的基于ou的ReRAM加速器为有效利用DNN稀疏性开辟了新的机会。我们提出了第一个实用的稀疏ReRAM引擎（SRE），它利用细粒度的基于ou的计算来共同利用权值和激活稀疏性。我们对广泛的神经网络模型进行的评估显示，在没有利用稀疏性的基线上，SRE提供了显著的性能加速（高达42.3倍）和能源节约（高达95.4%）。此外，SRE成功地实现了实际的基于ReRAM的DNN加速器设计，考虑到ReRAM单元可靠性的限制，实现了令人满意的推理精度，同时提供了与过度理想化对应的性能相当的性能和能源效率。





5 跑了一下gatys的代码，顺利运行了，有空去读一下源码，实现是tf1

![image-20221011211421137](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221011211421137.png)