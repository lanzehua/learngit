## 10.13 工作日记

---

1. 学习了python正则表达式机制。
2. 继续研读Gatys的工作的源码,以及稍微修改参数后观看运行结果。研究了每个argparse类里的参数对于全部文件的作用
3. 方向论文（AttnGAN  Fine-grained text to image generation with attentional generative adversarial networks）还得明天继续学



1 正则语法：![img](https://ai-studio-static-online.cdn.bcebos.com/fcb81141def94463bcc94d6e93b2a772f4fb7d298c45456ba6a78162b9a6f579)

python re库里的方法可以看这个[文章](https://aistudio.baidu.com/aistudio/projectdetail/573892?ad-from=1701)。

![image-20221013151601640](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013151601640.png)



2 ![image-20221013164105087](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013164105087.png)

1000epoch![image-20221013164125398](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013164125398.png)

100epoch![image-20221013164142393](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013164142393.png)

5kepoch；可以看到基本上1kepoch就可以满足人眼审美需求了；5k则是更为精细的还原了内容细节（跑了十六分钟。这个方法确实耗时）





 然后自己试图将loss画出来研究loss收敛在第几轮；出现了一点小问题![image-20221013175504213](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013175504213.png)

图片顺利生成了，但是没有绘制点。（使用的是作者的代码）![image-20221013184610523](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013184610523.png)

解决了。要综合使用多个参数。那个参数--print-iterations 控制图中的颗粒度。上图是10，下图5![image-20221013185022088](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013185022088.png)



3 论文（StackGan 提出了ca ；StackGan++ 提出了**color-consistency regularizatio** ； AttnGan）

3.1 StackGan：![image-20221013194451729](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013194451729.png)

![img](https://upload-images.jianshu.io/upload_images/14512145-a54dda21575ec856.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

![image-20221013201327155](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013201327155.png)

![image-20221013201436831](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013201436831.png)



3.2 StackGan++

本文认为StackGAN++的优势主要是以下三点：

虽然仍然是采用多阶段逐级提高生成图像的分辨率的方式，但是不同于之前的两阶段分开训练，StackGAN++可以采用end-to-end的方式进行训练
对于无条件图像生成提出了一种新的正则化方式color-consistency regularization来帮助在不同的分辨率下生成更一致的图像
既可以用于text-to-image这样的条件图像生成任务，也可以用于更一般的无条件图像生成任务，均可以取得比其他模型更优异的结果
模型整体的架构如下所示：
![img](https://img-blog.csdnimg.cn/20190628153945845.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZvcmxvZ2Vu,size_16,color_FFFFFF,t_70)

![image-20221013202624437](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013202624437.png)



3.3 AttnGan

在本文中，我们提出了一种注意生成对抗网络（AttnGAN），它允许由注意力驱动的、多阶段的细化来进行细粒度的文本到图像的生成。通过一种新的注意生成网络，AttnGAN可以通过关注自然语言描述中的相关词，在图像的不同子区域合成细粒度的细节。此外，还提出了一种深度注意多模态相似度模型来计算一个细粒度的图像-文本匹配损失来训练生成器。提出的AttnGAN显著优于之前的技术水平，在CUB数据集上的最佳报告初始分数提高了14.14%，在更具挑战性的COCO数据集上提高了170.25%。我们还通过可视化AttnGAN的注意层来进行详细的分析。这首次表明，分层注意GAN能够在单词级自动选择条件来生成图像的不同部分。

本文是StackGan++（StackGAN++:Realistic Image Synthesis with Stacked Generative Adversarial Networks]）的后续工作

![img](https://img-blog.csdnimg.cn/20181206020741960.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI5OTYzOTU3,size_16,color_FFFFFF,t_70)

在本文中作者提出了一个 Attentional Generative Ad-
versarial Network(AttnGAN),一种attention-driven的多stage的细粒度文本到图像生成器。

并借助一个深层注意多模态相似模型(deep attentional multimodal similarity model)来训练该生成器。

它首次表明 the layered attentional GAN 能够自动选择单词级别的condition来生成图像的不同部分。
![image-20221013211622797](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013211622797.png)



![image-20221013211648506](C:\Users\lzhrt\AppData\Roaming\Typora\typora-user-images\image-20221013211648506.png)





